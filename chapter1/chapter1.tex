\chapter{Introduction and Motivation}
\label{Ch-1:Sec:Introduction}

With the widespread collaborative spirit and the wide range usage of user-generated content, these concepts lead to the prosperity of wiki farms and make a great change to the ways of information retrieval and on-line education. Following the success and contribution of \href{https://en.wikipedia.org/wiki/Wikipedia}{Wikipedia} (January 15, 2001)\footnote{https://en.wikipedia.org/wiki/Wikipedia}, many other wiki hosting services were built in the past few years, like \href{https://www.fandom.com/}{Wikia} also known as Fandom (October 18, 2004)\footnote{https://www.fandom.com/}, \href{http://www.wikidot.com/}{Wikidot} (August 1, 2006)\footnote{http://www.wikidot.com/}, \href{https://www.huijiwiki.com/}{Huiji} (March 2015)\footnote{https://www.huijiwiki.com/}.\\ 
Compared with Wikipedia, like an encyclopedia, who has the \href{https://wiki.dbpedia.org/}{DBpedia}\footnote{https://wiki.dbpedia.org/}, a structured content collecting project from Wikipedia, many non-commercial and non-profit smaller wikis can go deeper into their specialty with more detailed information which can help the researchers find better supporting materials and open their minds with something they didn't know before. 
However, these wiki farms don't have enough money and developers (many of them are volunteers) to build a better search engine for their database and do the maintenances, which affects their high-quality data collections not used to its full potential, especially for some wikis of literatures and arts including much terminology, like the search engine used by Huiji based on \href{https://www.elastic.co/}{Elasticsearch}\footnote{https://www.elastic.co/}, based on the \href{http://lucene.apache.org/}{Lucene library}\footnote{http://lucene.apache.org/}. Elasticsearch is an excellent search engine, Lucene being a wonderful library as well, but somehow it still can't return the results we want, though the answers exactly appear in some pages. This situation becomes even worse when the size of wiki database grows sharply. \\
Facing the problem caused by an inefficient search engine and increasing data volume on those wiki farms, we decide to design another way for semi-structured data searching, combining the knowledge and skills learned in the past two years, which can also make some progresses in the Final Exam. These are the motivates and ideas the project begins with.\\
This work is called knowledge-based Q\&A system, which consists of the technology with Natural Language Processing, Graph Discovering, Python programming, Subgraph Isomorphism and Data Visualization. The most important two aims are 
\begin{itemize}
	\item Running the project automatically, for they may not have enough developers to rebuild the search engine project; and
	\item Terminology friendly, which is essential to those literary wikis with fictional languages, neologism and other self-created words created decades ago but not having a widely use in the real world.
\end{itemize}
This paper contains 7 sections organized as follows. In the first \nameref{Ch-1:Sec:Introduction} part, talking about the motivation of this project and what problem we need to be solved which are listed above. In the second part \nameref{Ch-2:Sec:Standardize}, giving the structure of graph dataset model, explaining how to extract information from plain texts, the processing of how to build it from semi-structured wiki data and verifying this graph can be used for natural language processing. In the third section \nameref{Ch-3:Sec:Extraction}, changing the natural language processing problem to SQL queries and letting the Q \& A system understand natural language questions. In the forth section \nameref{Ch-5:Sec:System}, combining the work from second to forth parts and describing the frame of the whole project with flow charts. In the sixth chapter \nameref{Ch-6:Sec:Experiment}, giving several scenarios to show how the modules work together. The last one is the conclusion of the project and some future works.
