\chapter{Introduction and Motivation}
\label{Ch-1:Sec:Introduction}
\section{Introduction and Motivation}
\vspace{-5pt}
With the widespread collaborative spirit, there is an increasing willingness to share the knowledge and skills on the Internet. One of the popular knowledge-sharing projects is called setting up wiki farms. Wiki farms, also known as Wiki Hosting Service, are servers installed core wiki code, which offer editors simpler tools to create and develop individual independent wikis\footnote{https://en.wikipedia.org/wiki/Wiki\_hosting\_service}. After the successful establishment of \href{https://en.wikipedia.org/wiki/Wikipedia}{Wikipedia} (January 15, 2001)\footnote{https://en.wikipedia.org/wiki/Wikipedia}, many other wiki hosting services were built in the past few years, like \href{https://www.fandom.com/}{Wikia} also known as Fandom (October 18, 2004)\footnote{https://www.fandom.com/}, \href{http://www.wikidot.com/}{Wikidot} (August 1, 2006)\footnote{http://www.wikidot.com/}, \href{https://www.huijiwiki.com/}{Huiji} (March 2015)\footnote{https://www.huijiwiki.com/}. The prosperity of wiki farms also brought a great transformation to the ways of information retrieval.\\
\indent Different from Wikipedia, well-known multi-language on-line encyclopedia, many small-scale, non-commercial and non-profit wikis can go deeper into their specialty with more detailed information which can help the researchers find better supporting materials and open their minds with something they didn't know before. However, these wiki farms usually don't have enough money and developers (volunteers) to go further into the optimization.\\
\indent Weak search engines of those wikis will affect their high-quality data collections not used to its full potential, especially for some literature and art wikis, which include a bunch of terminologies. For example, the search engine used by Huiji uses \href{https://www.elastic.co/}{Elasticsearch}\footnote{https://www.elastic.co/}, based on the \href{http://lucene.apache.org/}{Lucene library}\footnote{http://lucene.apache.org/}. Elasticsearch is an excellent search engine, and Lucene is a wonderful library as well. Though the answers exactly appear in some pages, somehow it still can't return the results we want, mainly caused by the terminology. This situation becomes even worse when the size of wiki database grows sharply. \\
\indent Facing the problem caused by an inefficient search engine and increasing data volume on those wiki farms, building a more powerful and efficient search engine for those wiki farms is an urgent matter. We decide to design another way for semi-structured data searching, which can help those small wiki farms easily build an efficient search engine and make better use of their editors' contributions.Thus, we build SEEKER, a knowledge-based Q\&A system for small wiki farms, which combines the tasks of Natural Language Processing, Graph Discovering, Python programming and Data Visualization. 

\section{Challenges}
\vspace{-5pt}
\noindent Among all the challenges we faced during the development, there are 5 important points:
\begin{itemize}
	\item \textbf{Modular programming}. This project can be easily embedded in small wiki farms. The only variable need to be changed is the domain of wiki farm. The data for building graphs can be updated every week or month. It is like a plug-in that won't influence the original data and functions.
	\item \textbf{Data cleaning}. Though crawling original data from wiki farm isn't a hard work while using API, there are still other problems we need to concern. Every editor has his or her own writing style. It is difficult to extract specific data from a large paragraph of text which could be used to build other data structures.
	\item \textbf{Keyword searching}. It is essential to those literary wikis which contains fictional languages, neologism and other self-created words appeared decades ago but not having a wide use in the real world. We have to make SEEKER terminology friendly.
	\item \textbf{Natural language processing}. This is the most difficult part in the project. We need to design a module which can understand the meaning of natural language queries.
	\item \textbf{Data visualization}. Plotting clean, tidy, readable, dynamic graphs is always a big issue in illustrating results to users. We need to find a suitable tool to make outputs attractive and interactive.
\end{itemize}

\section{Related Work}
\vspace{-5pt}
\label{Ch-2:Sec:Related Work}
In this paper, we build a graph based on the theory of Zesch's paper \cite{zesch2007analysis}. It is called Wikipedia Category Graph (WCG) which has a semantical network of articles building from related terms and a taxonomy-like structure of categories. The paper concludes that WCG is a scale-free, small-world graph like WordNet \cite{miller1998wordnet} which can be used for NLP tasks through a series of graph-theoretic analyses according to semantic relatedness (SR) measures. Thus, if $G$ meets the standard of those semantic networks, the NLP algorithms can be applied on this graph as it is described in the paper.\\
The other part is based on a project called Seq2SQL \cite{zhongSeq2SQL2017}. It is a deep neural network based on a Long Short-Term Memory network \cite{hochreiter1997long} which can translate natural language questions to corresponding SQL queries. It is trained by the WikiSQL, a dataset of hand-annotated examples of questions with their corresponding SQL, which gives rewards to learn a better policy to generate the query. Also, Seq2SQL can offer an advantage of the SQL structure and simplify the original problem.

\section{Structure of the Paper}
\vspace{-5pt}
This paper contains 6 chapters. Chapter~\ref{Ch-1:Sec:Introduction} talks about the motivation of setting up this program, the introduction of SEEKER and the problems need to be solved. Chapter~\ref{Ch-2:Sec:Related Work} shows the process of how to collect data, how to extract information and how to construct the Knowledge-graph from Text-corpus. Chapter~\ref{Ch-3:Sec:Extraction} discusses a method of teaching SEEKER understand natural language questions by changing the natural language questions to SQL queries. Chapter~\ref{Ch-5:Sec:System} combines the second and third chapters, which describes the frame of SEEKER with flow charts and the tools used for data visualization. Chapter~\ref{Ch-6:Sec:Experiment} gives four scenarios to explain the process of SEEKER intuitively and what kind of results we can get from the Q \& A system. The last chapter concludes the entire work and looks forward into the future development.